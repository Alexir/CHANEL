{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Valid_analysis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPZhUb5lK+ipKL09sTa3am/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexir/CHANEL/blob/master/Valid_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLCTAw__pPP8"
      },
      "source": [
        "## Authenticate `git` user\n",
        "You must have previously ensured that you enable github in your colab space:\n",
        "* In Colab, go to `Tools`->`Settings`; click `Request Github access...`\n",
        "* In Github, [generate a token](https://docs.github.com/en/enterprise/2.13/user/articles/creating-a-personal-access-token-for-the-command-line) to enable login under your account.\n",
        "* Use your Github id and the `token` and set the env vars. You can use the fancy lookup that I do or you can just type in your info (but be careful with the Sharing).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muIVGUW4ub42",
        "cellView": "form"
      },
      "source": [
        "#@title Access code & data: auth to github, mount GoogDrive\n",
        "# authenticate  ---> do this cell once when you start a session\n",
        "\n",
        "#mount your google drive, forcibly\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\",force_remount=True)\n",
        "\n",
        "# read git token from a file on your My Drive/\n",
        "import pandas as pd\n",
        "TOKEN = pd.read_csv( \"/content/drive/My Drive/CHANEL/DataAnalysis.txt\")\n",
        "# your git identity\n",
        "import os\n",
        "os.environ['GIT_TOKEN']=TOKEN.columns[0]\n",
        "os.environ['GIT_USER'] ='Alexir'  # <-- your git name here\n",
        "\n",
        "# ***NOTE that the above will randomly time out; you have to keep trying\n",
        "# or, just paste it in\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sVnRSobsPwg"
      },
      "source": [
        "## Set up environment and Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BbS4d_Zp-vv",
        "cellView": "form"
      },
      "source": [
        "#@title Set up environment\n",
        "import sys,os\n",
        "\n",
        "%pip install xmltodict\n",
        "%pip install stats\n",
        "\n",
        "import numpy as np\n",
        "import stats\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import xmltodict  # get the data tables\n",
        "\n",
        "pd.set_option('display.width', 180)\n",
        "\n",
        "## Data Preparation  **Fetch the repositories**\n",
        "# (which should have the current version of things).\n",
        "# The data here are from the `Data-Collection/` (i.e. RawEval) repository.\n",
        "\n",
        "# You need to have the `GIT_USER` and `GIT_TOKEN` env vars to be set (see previous step).\n",
        "# get a clean copy of the repository info\n",
        "%cd /content/\n",
        "%rm -fr Data-Collection ChatEval-AMT-Interface CHANEL\n",
        "!git --version\n",
        "!git  clone https://${GIT_USER}:${GIT_TOKEN}@github.com/CHANEL-JSALT-2020/Data-Collection\n",
        "!git  clone https://github.com/CHANEL-JSALT-2020/ChatEval-AMT-Interface \n",
        "\n",
        "!git  clone https://github.com/Alexir/CHANEL \n",
        "\n",
        "# linear fitting to duration data \n",
        "!pip install pwlf --upgrade --no-deps\n",
        "!pip install pyDOE --upgrade\n",
        "import pwlf\n",
        "\n",
        "# for agreement computations\n",
        "!pip install Krippendorff\n",
        "import krippendorff\n",
        "\n",
        "# convenience function\n",
        "# dataframe must have:  columns='i',index='c',values='k'\n",
        "def compute_ordinal_alpha(df) :\n",
        "  \"\"\" apply alpha() to a krip-formatted data frame \"\"\"\n",
        "  return(krippendorff.alpha(df.values, level_of_measurement='ordinal')\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "print('\\nReady')\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIueOe6eH5jJ",
        "cellView": "form"
      },
      "source": [
        "#@title Fetch (session, Raw, Eval) data from `.csv` \n",
        "# extract .pkl data into session and make tables\n",
        "\n",
        "# put put Metrics data up at /content level\n",
        "!mkdir /content/Metrics\n",
        "!mkdir /content/Metrics/Raw\n",
        "!mkdir /content/Metrics/Eval\n",
        "\n",
        "%cd /content/Data-Collection/\n",
        "!git checkout analysis  # fetch current code & data form (air)'s branch\n",
        "\n",
        "# unpickle the Raw data; make _all, _clean and _attn files\n",
        "!bash scripts/run_extraction.sh raws/030820_first_public.pkl work/\n",
        " # get Raw data into colab space, session data with durations\n",
        "!cp -p /content/Data-Collection/work/*.csv /content/Metrics/Raw/  \n",
        "!cp -p /content/CHANEL/Analysis_Valid/test_data/*-session.csv /content/Metrics/Raw/ \n",
        "# get the Eval data\n",
        "!cp -p /content/ChatEval-AMT-Interface/data/annotated/030820_first_public_finding_atts.csv   /content/Metrics/Eval/ # get Eval data\n",
        "\n",
        "print()\n",
        "!ls -l /content/Metrics/*/  # show data files\n",
        "\n",
        "# remember file paths (note column orders)\n",
        "RawAssignment_file = '/content/Metrics/Raw/'+ '030820_first_public-assigmt.csv'  # session data\n",
        "Session_file = '/content/CHANEL/Analysis_Valid/test_data/030820_first_public-session.csv' # w/ session durations\n",
        "RawData_file =  '/content/Metrics/Raw/'+  \"030820_first_public-raw.csv\"   # [i,c,k]\n",
        "EvalData_file = '/content/Metrics/Eval/'+ \"030820_first_public_finding_atts.csv\"   # [i,k,c]\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZdwgav8u3hv",
        "cellView": "form"
      },
      "source": [
        "#@title Process data into local formats \n",
        "# -- column order\n",
        "# -- filter out catch trials\n",
        "# note that Raw and Eval have different col orders\n",
        "\n",
        "# prepare RawData and breakouts\n",
        "RawData_all = pd.read_csv(RawData_file,names=['i','c','k'],delimiter='\\t') # alpha() dim names \n",
        "RawData_all['k'] = RawData_all['k'].astype(np.float64)  # fix number type\n",
        "RawData_attn =  RawData_all[  RawData_all['i'].str.contains(\"ATTENTION\")] \n",
        "RawData_clean = RawData_all[ ~RawData_all['i'].str.contains(\"ATTENTION\")] \n",
        "\n",
        "# prepare EvalData and breakouts\n",
        "ata = pd.read_csv(EvalData_file,names=['i','k','c']) # alpha() dim names \n",
        "hedless = ata.drop(index=[0])  #drop the header line  \n",
        "EvalData_all = hedless[['i','c','k']]   # nltk-wise column order\n",
        "EvalData_all['k'] = EvalData_all['k'].astype(np.float64)  # fix number type\n",
        "EvalData_attn =  EvalData_all[  EvalData_all['i'].str.contains('ATTENTION')] \n",
        "EvalData_clean = EvalData_all[ ~EvalData_all['i'].str.contains('ATTENTION')] \n",
        "\n",
        "# prepare  (Raw) Session data; there is no Eval session data\n",
        "RawSession = pd.read_csv(Session_file, delimiter='\\t', header=0 )\n",
        "\n",
        "\n",
        "print('done')\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED40azXQNuYY"
      },
      "source": [
        "##Compare with `krippendorff_test.ipnb`\n",
        "Is the data format handling consistent? If yes, the alphas should be the same. This a sanity check..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAHvTMNa9IwF",
        "cellView": "form"
      },
      "source": [
        "#@title Krippendorff validation (previous computation)\n",
        "#@markup check whether the data in the two notebooks are the same, by computing alpha\n",
        "#\n",
        "\n",
        "# make Krippendorff-style formats; check w/ other notebook, to be the same\n",
        "# PIVOT into correct format: [c,k,i] --> [c,i] /k\n",
        "RawData_krip_all =   pd.pivot_table(RawData_all,  columns='i',index='c',values='k')\n",
        "RawData_krip_attn  = pd.pivot_table(RawData_attn, columns='i',index='c',values='k')\n",
        "RawData_krip_clean = pd.pivot_table(RawData_clean,columns='i',index='c',values='k')\n",
        "\n",
        "EvalData_krip_all =   pd.pivot(EvalData_all,  columns='i',index='c',values='k')\n",
        "EvalData_krip_attn  = pd.pivot(EvalData_attn, columns='i',index='c',values='k')\n",
        "EvalData_krip_clean = pd.pivot(EvalData_clean,columns='i',index='c',values='k')\n",
        "\n",
        "# compute alphas: number should match...\n",
        "print('\"clean\" subset:')\n",
        "print('RawData ordinal  [0.321]:', round(compute_ordinal_alpha(RawData_krip_clean),3))\n",
        "print('EvalData ordinal [0.389]:', round(compute_ordinal_alpha(EvalData_krip_clean),3))\n",
        "\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnnOvlZyrfKx"
      },
      "source": [
        "## Basic stats for Raw, Eval\n",
        "\n",
        "Various metrics that allow the data to be characterized in useful ways including:\n",
        "* For a deployment what is the profile of hit accepts?\n",
        "* distribution of session durations\n",
        "* incidence of poor attention (check trials)\n",
        "* corpus\n",
        "* etc\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Key to column names: (Kippendorff nomenclature --- Metrics data format)\n",
        "* `c --- WorkerId`\n",
        "* `i --- Excerpts`\n",
        "* `k --- Rating`\n",
        "* `H --- hit`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDyh_M4wrk_-",
        "cellView": "form"
      },
      "source": [
        "#@title pull Raw, Eval data into local space, drop check trials\n",
        "# \n",
        "\n",
        "Raw_sessions = pd.read_csv(RawAssignment_file,delimiter='\\t')\n",
        "print('raw sessions information: \\n',Raw_sessions.shape,Raw_sessions.columns,'\\n' )\n",
        "\n",
        "Raw_data = pd.read_csv(RawData_file, delimiter='\\t', names=['i','c','k']) \n",
        "Raw_data = Raw_data[ ~Raw_data['i'].str.contains('ATTENTION')]  # remove\n",
        "#print(Raw_data[0:2])\n",
        "\n",
        "# the Eval data has a header. Also remove catch trials \n",
        "Eval_data = pd.DataFrame()\n",
        "ata = pd.read_csv(EvalData_file,names=['i','k','c'])\n",
        "hedless = ata.drop(index=[0])  # drop the header line  \n",
        "Eval_hless = hedless[['i','c','k']]   # make column order same as Raw\n",
        "Eval_hless.dropna(subset=['i'],inplace=True) # why was this needed? ***\n",
        "Eval_data = Eval_hless[ ~Eval_hless['i'].str.contains('ATTENTION')] \n",
        "Eval_data['k'] = Eval_data['k'].astype(int)  # why was *this* needed?\n",
        "# print(Raw_data[0:6],'\\n',Eval_data[0:6])\n",
        "\n",
        "print()\n",
        "print('raw excerpts: ',Raw_data.shape,Raw_data.columns)\n",
        "print('eval excerpts:',Eval_data.shape,Eval_data.columns)\n",
        "print()\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8_oAE4PZum7",
        "cellView": "form"
      },
      "source": [
        "#@title compute session durations (Raw data)\n",
        "# this only works for the Raw_data (for which session data is available)\n",
        "workers = pd.DataFrame(columns=['WorkerId',\n",
        "                                'Sessions',   # sessions attempted\n",
        "                                'Excerpts',   # excerpts completed\n",
        "                                'FirstTime',  # when engagement began\n",
        "                                'LastTime',   # when it ended\n",
        "                                'Duration']\n",
        ")\n",
        "\n",
        "workers['WorkerId'] = pd.unique(Raw_data['c'])  # get unique Worker_Id's\n",
        "print('number of workers:  ',len(workers['WorkerId']))\n",
        "\n",
        "# compute counts per worker\n",
        "session_cnt = Raw_sessions['WorkerId'].value_counts()  # count up instances of sessions,\n",
        "excerpt_cnt = Raw_data['c'].value_counts()  # count up instances of excerpts\n",
        "  \n",
        "# go thru worker list, get index for a unique workerId; insert computed counts\n",
        "for a in session_cnt.index :\n",
        "  workers['Sessions'][ workers[workers['WorkerId']==a].index ] = session_cnt[a]\n",
        "  workers['Excerpts'][ workers[workers['WorkerId']==a].index ] = excerpt_cnt[a]\n",
        "\n",
        "# get session times; for each worker their first sign-on, last one, and delta\n",
        "for wrkr_idx in workers['WorkerId'] :\n",
        "  A_time = Raw_sessions[Raw_sessions['WorkerId']==wrkr_idx]['AcceptTime']  # get vectors of times \n",
        "  S_time = Raw_sessions[Raw_sessions['WorkerId']==wrkr_idx]['SubmitTime']\n",
        "\n",
        "  # get min/max and duration of engagement for each worker\n",
        "  workers.loc[workers.WorkerId == wrkr_idx, 'FirstTime']   = pd.to_datetime(A_time.min())\n",
        "  workers.loc[workers.WorkerId == wrkr_idx, 'LastTime']   = pd.to_datetime(S_time.max())\n",
        "  workers.loc[workers.WorkerId == wrkr_idx, 'Duration'] = workers['LastTime'] - workers['FirstTime']\n",
        "  #print(workers.loc[workers.WorkerId == wrkr_idx, 'Duration'])\n",
        "\n",
        "# global durs\n",
        "print('min engagement time:', workers['Duration'].min())\n",
        "print('max engagement time:', workers['Duration'].max())\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-RREfLXkcIr",
        "cellView": "form"
      },
      "source": [
        "#@title Cumulative items per worker, Raw data (ordered by productivity)\n",
        "xx = session_cnt.sort_values(ascending=False).cumsum().plot() \n",
        "# (not sure why this is interesting; but it's a 1 liner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Steric8TmBuc",
        "cellView": "form"
      },
      "source": [
        "#@title Stats for the (Raw) session file\n",
        "#@markdown \n",
        "maxi = workers.Excerpts.max()\n",
        "print('max sessions:', workers['Sessions'].max())\n",
        "print('min excepts: ', workers['Excerpts'].min())\n",
        "print('max exerpts: ', workers['Excerpts'].max())\n",
        "print()\n",
        "\n",
        "# assume return is a single row; get 0'th value, the worker id\n",
        "maxw = workers.loc[workers.Excerpts == workers['Excerpts'].max()].values[0][0]\n",
        "# variance for the worker who worked the hardest\n",
        "vari = Raw_data.loc[Raw_data['c'] == maxw].var(skipna=True).values[0]\n",
        "print(\"max()'s var:\",round(vari,3),' (for hardest worker)')\n",
        "#\n",
        "\n",
        "#workers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdVpfvsts5Cn",
        "cellView": "form"
      },
      "source": [
        "#@title Distribution of HITs accepted, over time\n",
        "# much action at the start: it's the new hit on the block \n",
        "# steady state thereafter then it peters out; did the repeaters use up their quota?\n",
        "   \n",
        "Raw_sessions['date'] = Raw_sessions['AcceptTime'].astype(\"datetime64\")\n",
        "print('range:',[ Raw_sessions['date'].min(), Raw_sessions['date'].max() ], '\\nengagement:',Raw_sessions['date'].max() -  Raw_sessions['date'].min(),'\\n');\n",
        "print('distribution:',Raw_sessions['date'].hist(bins=36,xrot=45));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsuFXG4w1BVh",
        "cellView": "form"
      },
      "source": [
        "#@title compute session durations; (->TimeDelta)\n",
        "\n",
        "Raw_sessions['duration'] = Raw_sessions['SubmitTime'].astype(\"datetime64\") - Raw_sessions['AcceptTime'].astype(\"datetime64\")\n",
        "# plot\n",
        "foo = pd.to_timedelta(Raw_sessions['duration'],unit='s').astype('timedelta64[s]') #/60.0\n",
        "foo.hist(bins=60)\n",
        "print(f'quantile(25,50,75,90): {foo.quantile(0.25):.0f} {foo.quantile(0.5):.0f} {foo.quantile(0.75):.0f} {foo.quantile(0.90):.0f} (sec)')\n",
        "print(f'mean: {foo.mean():.2f}sec - {(foo.mean()/60.0):.2f}min')\n",
        "print(f'mode: {foo.mode().mean():.1f}sec')\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qWKUxb0Q5JV",
        "cellView": "form"
      },
      "source": [
        "#@title Histo for ratings in catch trials\n",
        "#  does not look so good. \n",
        "\n",
        "# Need to have info about individual items: Is it the people or the items?\n",
        "chksum = Raw_sessions['CHECK'].count()\n",
        "print(f'count: {chksum} ')\n",
        "histog = pd.cut(Raw_sessions['CHECK'],4).value_counts()\n",
        "# print(type (histog.divide(chksum)))\n",
        "hout = histog.divide(chksum)\n",
        "print (f'{hout}')\n",
        "Raw_sessions['CHECK'].hist();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_0xeCINsfHj"
      },
      "source": [
        "## Item Quality; basic stats\n",
        "Extracts should show some consistency of markup across annotators. If not, there might be inherent problems in either selection or instructures. Identify such."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lznaOY6yd0YR",
        "cellView": "form"
      },
      "source": [
        "#@title Compute and plot item variances (unsorted view)\n",
        "import matplotlib.pyplot as plt \n",
        "from pprint import pprint\n",
        "\n",
        "#print(Raw_data.describe)\n",
        "y_raw  = pd.pivot_table(Raw_data,  columns='c',index='i',values='k')\n",
        "y_eval = pd.pivot_table(Eval_data, columns='c',index='i',values='k')\n",
        "\n",
        "# collect variances into an array\n",
        "datah_raw  = []\n",
        "datah_eval = []\n",
        "\n",
        "# collect item details into obs_*\n",
        "obs_raw =  pd.DataFrame( index=y_raw.index,  columns=['var','obs','wrk','cnt'] )\n",
        "obs_eval = pd.DataFrame( index=y_eval.index, columns=['var','obs','wrk','cnt'])\n",
        "\n",
        "# in each item (row index), use jusy non-Nan values (i.e. across all workers)\n",
        "for item in y_raw.index   : \n",
        "  var = y_raw.loc[item].var(skipna=True)  # using only real values\n",
        "  obs_raw.at[item,'var'] = var \n",
        "  if pd.isna(obs_raw.at[item,'obs']) : obs_raw.at[item,'obs'] = []\n",
        "  if pd.isna(obs_raw.at[item,'wrk']) : obs_raw.at[item,'wrk'] = []\n",
        "  obs_raw.at[item,'obs'] = list( RawData_all.loc[RawData_all['i']==item]['k'] )\n",
        "  obs_raw.at[item,'wrk'] = list( RawData_all.loc[RawData_all['i']==item]['c'] )\n",
        "  obs_raw.at[item,'cnt'] = len( obs_raw.at[item,'wrk'] )\n",
        "  datah_raw.append(var)  # for plots\n",
        "\n",
        "for item in y_eval.index   : \n",
        "  var = y_eval.loc[item].var(skipna=True)  # a row or ratings\n",
        "  obs_eval.at[item,'var'] = var \n",
        "  if pd.isna(obs_eval.at[item,'obs']) : obs_eval.at[item,'obs'] = []\n",
        "  if pd.isna(obs_eval.at[item,'wrk']) : obs_eval.at[item,'wrk'] = []\n",
        "  obs_eval.at[item,'obs'] = list(EvalData_all.loc[EvalData_all['i']==item]['k'])\n",
        "  obs_eval.at[item,'wrk'] = list(EvalData_all.loc[EvalData_all['i']==item]['c'])\n",
        "  obs_eval.at[item,'cnt'] = len(obs_eval.at[item,'wrk'])\n",
        "  datah_eval.append(var)\n",
        "\n",
        "print('obs_raw and obs_eval created.\\n')\n",
        "\n",
        "\n",
        "# Show item scatter across variances (not useful, but why not?)\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(121)\n",
        "plt.ylabel('variance')\n",
        "plt.xlabel('item')\n",
        "plt.title('Raw')\n",
        "plt.xlim(-50,800)\n",
        "_= plt.plot(datah_raw)\n",
        "plt.subplot(122)\n",
        "plt.ylabel('variance')\n",
        "plt.xlabel('item')\n",
        "plt.title('Eval')\n",
        "plt.xlim(-50,800)\n",
        "_= plt.plot(datah_eval)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "print()\n",
        "print('raw mean: ', round(mean(datah_raw),6) )\n",
        "print('eval mean:', round(mean(datah_eval),6) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KLsSnut58kh"
      },
      "source": [
        "# inspect datasets\n",
        "\n",
        "cnt = 0\n",
        "for i in obs_eval.index : \n",
        "  cnt+=1\n",
        "  if (cnt<10) : continue\n",
        "  print('\\n',cnt,i,'------------ \\nRawData_all',RawData_all.loc[RawData_all['i']==i])\n",
        "  print('* obs_eval\\n',obs_eval.loc[i])\n",
        "  #if obs_eval.loc[i]['cnt'] !=5 : print (obs_eval.loc[i])\n",
        "  if (cnt>12) : break\n",
        "print('---',RawData_all.loc[RawData_all['i']=='MPATHY-000003-0004'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnVE7gvLqBmb"
      },
      "source": [
        "# inspect datasets\n",
        "\n",
        "#RawData_all.describe,\n",
        "temp = RawData_all.loc[RawData_all['i']=='MPATHY-000025-0003']\n",
        "#temp = y_raw.loc['MPATHY-000025-0003'].notna()\n",
        "print(temp)\n",
        "print()\n",
        "foo = []\n",
        "for t in RawData_all.loc[RawData_all['i']=='MPATHY-000025-0003'].index :\n",
        "  foo.append([temp.at[t,'c'],temp.at[t,'k']])\n",
        "pprint(foo)\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPs8Vs1aLUUm",
        "cellView": "form"
      },
      "source": [
        "#@title Plot cumulative item counts against variance\n",
        "# the count can be used to chose a variance cutoff.\n",
        "\n",
        "h_r = pd.DataFrame({'items': np.arange(0,len(datah_raw)), \n",
        "                    'vars':  np.sort(datah_raw) })\n",
        "h_e = pd.DataFrame({'items': np.arange(0,len(datah_eval)), \n",
        "                    'vars':  np.sort(datah_eval) })\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.xlabel('variance')\n",
        "plt.ylabel('item count')\n",
        "plt.xlim(-0.3,3.0)\n",
        "plt.ylim(-20,800)\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Raw cumulative Variance')\n",
        "plt.ylim(-20,800)\n",
        "plt.axis([0.0,3.0,0,800])\n",
        "plt.grid(True)\n",
        "plt.plot(h_r['vars'],h_r['items']);\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Eval cumulative Variance')\n",
        "plt.ylim(-20,800)\n",
        "plt.axis([0.0,3.0,0,800])\n",
        "plt.grid(True)\n",
        "plt.plot(h_e['vars'],h_e['items']);\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\"\"\" [SEE NEXT CELL]\n",
        "# chart showing effect of diferent cutoffs (see next cell for better ino)\n",
        "print('\\n% cutoff Raw   % cutoff Eval')\n",
        "for i in range(0,751,50) : \n",
        "  if i < 601 :\n",
        "    print('{:3d} {:6.2f}'.format(i,round(i/6.0,2)),end='')\n",
        "    print('     {:3d} {:6.2f}'.format(i,round(i/7.5,2)))\n",
        "  else: \n",
        "    print(' '*15,end='')\n",
        "    print('{:3d} {:6.2f}'.format(i,round(i/7.5,2)))\n",
        "\"\"\"\n",
        "pass\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oogdeEYkzl7",
        "cellView": "form"
      },
      "source": [
        "#@title compute yield of items, given a threshold\n",
        "#@markdown make lists of all `good` and `bad` items at a given threshold.\n",
        "#@markdown `{raw,eval}_{good,bad}`\n",
        "# we want enough data left over...\n",
        "\n",
        "# display yield, collect item id's\n",
        "raw_good = {} # item id's by var\n",
        "raw_bad  = {} # item id's by var\n",
        "eval_good = {} # item id's by var\n",
        "eval_bad  = {} # item id's by var\n",
        "for thresh in np.arange(3.0,-0.25,-0.25) :\n",
        "\n",
        "  short_raw  = [] # for table...\n",
        "  for item in obs_raw.index :  # (item,var) table\n",
        "    if obs_raw.at[item,'var'] < thresh :\n",
        "      short_raw.append(obs_raw.at[item,'var']) # collect \"good\" variances\n",
        "      if not thresh in raw_good : raw_good[thresh] = [] # and the good items\n",
        "      raw_good[thresh].append(item)\n",
        "    else: # save the reject item id's\n",
        "      if not thresh in raw_bad : raw_bad[thresh] = []\n",
        "      raw_bad[thresh].append(item)\n",
        "\n",
        "  short_eval  = [] # for table...\n",
        "  for item in obs_eval.index :  # (item,var) table\n",
        "    if obs_eval.at[item,'var'] < thresh :\n",
        "      short_eval.append(obs_eval.at[item,'var']) # collect \"good\" variances\n",
        "      if not thresh in eval_good : eval_good[thresh] = [] # and the good items\n",
        "      eval_good[thresh].append(item)\n",
        "    else: # save the reject item id's\n",
        "      if not thresh in eval_bad : eval_bad[thresh] = []\n",
        "      eval_bad[thresh].append(item)\n",
        "\n",
        "  # the threshold lists are used to compute yields\n",
        "  #print('<',thresh,'\\t',end='')\n",
        "  print ( 'raw:  {}<{:4.2f}= {:3d}'.format(len(datah_raw),thresh, len(short_raw)), \n",
        "       ' \\tkeeps->{:6.1f}%'.format(100.0*(len(short_raw)/len(datah_raw))),'\\t',end='' )\n",
        "  print ( '\\teval: {}<{:4.2f}= {:3d}'.format(len(datah_eval),thresh, len(short_eval)), \n",
        "       '\\tkeeps->{:6.1f}%'.format(100.0*(len(short_eval)/len(datah_eval)) ))\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXHS43WsIRTR",
        "cellView": "form"
      },
      "source": [
        "#@title Sanity check \n",
        "print(len(datah_raw),'\\t bad\\t good\\t sum')\n",
        "for k in raw_bad.keys() :  \n",
        "  if k == 0.0 : continue\n",
        "  print(k,'\\t',len(raw_bad[k]),'\\t',\n",
        "        len(raw_good[k]),'\\t',len(raw_bad[k])+len(raw_good[k]))\n",
        "print()\n",
        "print(len(datah_eval),'\\t bad\\t good\\t sum')\n",
        "for k in eval_bad.keys() : \n",
        "  if k == 0.0 : continue\n",
        "  print(k,'\\t',len(eval_bad[k]),'\\t',\n",
        "        len(eval_good[k]),'\\t',len(eval_bad[k])+len(eval_good[k]))\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVbKjVT9KsS"
      },
      "source": [
        "#Do subsets per threshold and compute alphas\n",
        "These number appear comparable to \n",
        "the worker-based thresholding but perhaps stricter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF2LzVfn8mBd",
        "cellView": "form"
      },
      "source": [
        "#@title [ set up vars ]\n",
        "raw_plot  = []      # alpha list for plot\n",
        "eval_plot = []\n",
        "raw_plot_cnt  = []  # count of items supporting an alpha\n",
        "eval_plot_cnt = []\n",
        "raw_plot_perc  = [] # counts, as percentages\n",
        "eval_plot_perc = []\n",
        "x_plot = []         # x-axis values\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LW7VGuS8hy9",
        "cellView": "form"
      },
      "source": [
        "#@title  print out the datapoints\n",
        "if ( True ) :\n",
        "  print('Raw Data:')\n",
        "  raw_alpha = {}\n",
        "  for k in raw_good.keys() :\n",
        "    if not k in raw_alpha : raw_alpha[k] = pd.DataFrame() \n",
        "    raw_alpha[k] = RawData_krip_all[raw_good[k]]\n",
        "    alpha = compute_ordinal_alpha(raw_alpha[k])\n",
        "    print('thresh: {:4.2f} {:3d} {:6.1f}%\\talpha: {:4.2f}'.format(k,len(raw_good[k]),\n",
        "                                                            100.0*(len(raw_good[k])/len(obs_raw.index)),\n",
        "                                                            round(alpha,2)) )\n",
        "    raw_plot.append(alpha)\n",
        "    raw_plot_cnt.append(len(raw_good[k]))\n",
        "    raw_plot_perc.append(100.0*(len(raw_good[k])/len(obs_raw.index)))\n",
        "    x_plot.append(k)\n",
        "\n",
        "  print()\n",
        "  print('Eval Data:')\n",
        "  eval_alpha = {}\n",
        "  for k in eval_good.keys() :\n",
        "    if not k in eval_alpha : eval_alpha[k] = pd.DataFrame() \n",
        "    eval_alpha[k] = EvalData_krip_all[eval_good[k]]\n",
        "    alpha = compute_ordinal_alpha(eval_alpha[k])\n",
        "    print('thresh: {:4.2f} {:3d} {:6.1f}%\\talpha: {:4.2f}'.format(k,len(eval_good[k]),\n",
        "                                                            100.0*(len(eval_good[k])/len(obs_eval.index)),\n",
        "                                                            round(alpha,2)) )\n",
        "    eval_plot.append(alpha)\n",
        "    eval_plot_cnt.append(len(eval_good[k]))\n",
        "    eval_plot_perc.append(100.0*(len(eval_good[k])/len(obs_eval.index)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIqiuL_HXuBD",
        "cellView": "form"
      },
      "source": [
        "#@title plot alpha curves...\n",
        "print()\n",
        "#for i in range(len(x_plot)) :   print(i,x_plot[i])\n",
        "#  print(x_plot[i],'\\t', raw_plot[i],'\\t',eval_plot[i])\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.suptitle('effect of dropping high-variance items',fontsize=16)\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.title('alpha by threshold')\n",
        "plt.ylim(-0.20,0.700)\n",
        "plt.axis([3.2,0.0,  0.2,0.7])\n",
        "# plt.yscale('log')\n",
        "plt.grid(True)\n",
        "plt.plot(x_plot,raw_plot,'b-o',label='raw')\n",
        "plt.plot(x_plot,eval_plot,'r-o',label='eval')\n",
        "plt.xlabel('threshold (on variance)')\n",
        "plt.ylabel('alpha')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.title('item yield for given threshold')\n",
        "plt.ylim(-0.20,     100,800)\n",
        "plt.axis([3.2,0.0,  100,800])\n",
        "plt.grid(True)\n",
        "plt.plot(x_plot,raw_plot_cnt,'b-o',label='raw')\n",
        "plt.plot(x_plot,eval_plot_cnt,'r-o',label='eval')\n",
        "plt.xlabel('threshold (on variance)')\n",
        "plt.ylabel('count')\n",
        "plt.legend(['raw','eval'])\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.title('% item yield')\n",
        "plt.ylim(-0.20,     15,110)\n",
        "plt.axis([3.2,0.0,  15,110])\n",
        "plt.grid(True)\n",
        "plt.plot(x_plot,raw_plot_perc,'b-o',label='raw')\n",
        "plt.plot(x_plot,eval_plot_perc,'r-o',label='eval')\n",
        "plt.xlabel('threshold (on variance)')\n",
        "plt.ylabel('count')\n",
        "plt.legend(['raw','eval'])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_yZoA2q-a-d"
      },
      "source": [
        "Tentative variance threshold:   ` < 1.75`\n",
        "* The point at which alpha starts to improve.without lower yield too much, but yield is still reasonable (~90%)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1C5wSNOONrY"
      },
      "source": [
        "## Filtering criteria based on worker behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akp5zRlH_hj-",
        "cellView": "code"
      },
      "source": [
        "#@title Removing workers that \"fail\" the check trial\n",
        "#@markdown proposed thresh at 2.0; random rating would be 2.5\n",
        "# 4.0 -> ignores or misunderstands task\n",
        "# 1.0 -> scrupulously \"correct\" (as per developer selection)\n",
        "#\n",
        "\n",
        "# get summary stats per worker: session count; means for dur, CHECK, duration\n",
        "Times = pd.DataFrame()\n",
        "Times['count'] = Raw_sessions.groupby(\"WorkerId\")['WorkerId'].count()\n",
        "Times['duration'] = Raw_sessions.groupby('WorkerId').duration.agg(lambda x:x.mean()).to_frame()\n",
        "Times['secs'] = Times['duration'].dt.seconds\n",
        "Times['CHECK_mean'] = Raw_sessions.groupby('WorkerId')['CHECK'].mean()\n",
        "\n",
        "# compute yield for various worker CHECK thresholds\n",
        "check_thresh = np.arange(1.0,4.25,0.25)\n",
        "x_check      = np.arange(1.0,4.0,0.25)  # for plot\n",
        "\n",
        "Check_yield = pd.DataFrame(columns=['count'], index=check_thresh)\n",
        "bogons = {}\n",
        "Keepers = {}\n",
        "print('\\tt\\tKeeper\\tCheck_y\\tbogons')\n",
        "for t in check_thresh :  \n",
        "  print(t,end='\\t')\n",
        "  # counts for <= each threshold...\n",
        "  bogons[t] = Times.query( \"CHECK_mean <= @t\" )\n",
        "  Check_yield['count'][t] = len(Times.loc[Times['CHECK_mean'] >= t])\n",
        "  # counts for > each thresh\n",
        "  Keepers[t] = Times.query( \"CHECK_mean >= @t\" )  # rolls over to next section\n",
        "  print(t,'\\t',len(Keepers[t]),'\\t',Check_yield['count'][t],'\\t',len(bogons[t]))\n",
        "\n",
        "# display yield (higher thresh the more you keep)\n",
        "plt.figure(figsize=[5.0,5.0])\n",
        "plt.plot( check_thresh, Check_yield['count'] )  # plot\n",
        "plt.grid()\n",
        "plt.xlim(4,1)\n",
        "plt.ylim(10,100)\n",
        "plt.axis([4.5,0.5,  0,100])\n",
        "plt.title('Mean score on check trials, by worker')\n",
        "plt.xlabel('check score threshold')\n",
        "plt.ylabel('yield (% workers)')\n",
        "plt.show()\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrvx9Gie3e08",
        "cellView": "code"
      },
      "source": [
        "#@title Filter  workers per threshold to get \"good\" workers; get alphas\n",
        "#@markdown This doesn't work when there's too many all NaN items.\n",
        "#@markdown We can filter but then some items have to be re-collected.\n",
        "#@markdown On the positive side, a thresh of 1.5 gets us to alpha ~= 0.59\n",
        "#@markdown\n",
        "#@markdown Things flatten out at ~2.75, probably because there are no new workers in this range. (See previous cell.)\n",
        "#@markdown There's a blip up when the 4.0 people show up.\n",
        "#@markdown\n",
        "#@markdown Note that the all-data alpha is the same as for the earlier validation run. Lucky us.\n",
        "\n",
        "# assemble df's   per thresh\n",
        "Keeper_alpha = {}\n",
        "Loser_items = pd.DataFrame(index=RawData_krip_clean.index)\n",
        "print(\"thresh\\titems\\tworkrs\\t alpha\")\n",
        "for t in check_thresh :  # compute alpha for each threshold\n",
        "\n",
        "  # pull out all worker data for those in the current tranche\n",
        "  temp = RawData_krip_clean[RawData_krip_clean.index.isin(Keepers[t].index)]\n",
        "  # compute some coverage stats  \n",
        "  item = 0\n",
        "  for i in temp.columns :  # over all workers, count # of items done\n",
        "    if int(temp[i].count()) > 0 : item += 1 \n",
        "  \n",
        "  #over all items, count how many workers did it (in this threshold set)\n",
        "  cnt = 0\n",
        "  for j in temp.columns :  # go thru workers in this set\n",
        "    for i in temp.index : \n",
        "      if np.isnan(temp.loc[i,j]) : Loser_items[i] = 1\n",
        "      else: cnt += 1\n",
        "\n",
        "  # finally\n",
        "  Keeper_alpha[t] = compute_ordinal_alpha(temp)\n",
        "  print(\"{:5.2f}\\t{}\\t{}\\t{:6.3f}\".format(t,item,len(temp),Keeper_alpha[t]))\n",
        "print()\n",
        "\n",
        "# plot the result; alpha (higher thresh the more you keep)\n",
        "plt.figure(figsize=[5.0,5.0])\n",
        "plt.plot( list(Keeper_alpha.keys()), list(Keeper_alpha.values()), lw=3)\n",
        "plt.grid()\n",
        "plt.xlim(4.5,1.5)\n",
        "plt.ylim(-0.2,1.0)\n",
        "plt.axis([0.5,4.5,  -0.2,1.0])\n",
        "plt.title('Alpha, removing workers that fail CHECK')\n",
        "plt.xlabel('CHECK threshold')\n",
        "plt.ylabel('alpha')\n",
        "plt.show()\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_rAHgri8kaQ"
      },
      "source": [
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1UXb2A091Xn"
      },
      "source": [
        "# what is actually in Keepers?\n",
        "foo = Keepers[4.0].index  # list of turkers that pass thresh\n",
        "items = temp.columns  # list of all items\n",
        "for c in foo :\n",
        "  print(c)\n",
        "  for i in items :\n",
        "    if pd.notnull(temp.loc[c,i]) :\n",
        "      print( (c,i),temp.loc[c,i] )\n",
        "#  if temp.loc[ foo[2],i] \n",
        "#  print(temp.loc[ foo[2],i])\n",
        "  # df1.loc['a', 'A']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Z6A1yn-LgA"
      },
      "source": [
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ1GvWtp9H1G"
      },
      "source": [
        "len(Keepers[2.0]), Keepers[2.0].describe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwUzYWdD7Ia_"
      },
      "source": [
        "c,i,  temp.columns[0] ,foo,len(foo) , temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idhs1qXc14Cw"
      },
      "source": [
        " foo,items[2],'\\n',temp[temp[items[2]].notna()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bYPkSR48zwN",
        "cellView": "both"
      },
      "source": [
        "#@title Mean session duration as threshold\n",
        "#@markdown This might take a while to compute; \n",
        "#@markdown sometimes the model fit barfs and there's an error. \n",
        "#@markdown But the right result still seems to come out.\n",
        "#@markdown \n",
        "#@markdown Judging from the breakpoints, dropping the fastest 11 workers seem to get us beyond the bogus sessions.\n",
        "#@markdown \n",
        "\n",
        "\n",
        "# make series\n",
        "x = np.arange(0,95,1,dtype=float)\n",
        "y = np.array(Times['secs'].sort_values(),dtype=float)\n",
        "\n",
        "# build model (<5 breaks seems to brreak the code)\n",
        "model = pwlf.PiecewiseLinFit(x, y)\n",
        "breaks = model.fit(6)\n",
        "# build the piece-wise lines into a plot\n",
        "x_hat = np.linspace(x.min(), x.max(),num=95)\n",
        "y_hat = model.predict(x_hat)\n",
        "\n",
        "plt.figure(figsize=[5.0,5.0])\n",
        "plt.plot(x, y,'b +')\n",
        "plt.plot(x_hat, y_hat, 'r--',lw=2)\n",
        "plt.grid()\n",
        "plt.xlabel('yield (/95)')\n",
        "plt.ylabel('session duration (sec)')\n",
        "plt.show()\n",
        "\n",
        "print('breakpoints')\n",
        "print('| dur (sec) | workrs |')\n",
        "print('| --- | --- |')\n",
        "for x in breaks :\n",
        "  print('|    {:3.0f} | {:3d} |'.format(y[int(x)],int(x)+1))\n",
        "print()\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}